---
title: Knowledge Discovery DRAFT
subtitle: Knowledge Discovery based on the IPBES Assessments
date: today
author:
  - name: Rainer M Krug 
    id: rmk
    orcid: 1234-1234-1234-1234
    email: Rainer@krugs.de
    affiliation: 
      - name: University of Zürich
        city: Zürich
        state: ZH
        url: www.uzh.ch
    roles: [author, editor]
abstract: > 
  The assessments Linked Open Data is used to conduct analysis and searches 
  to discover new knowledge related to the assessmwents.
# keywords:
#   - aaaaa
#   - bbbbb
license: "CC BY"
citation: 
  type: report
  doi: xxxxxxxxxxxxxx
doi: xxxxxxxxxxxxxx4
version: 0.0.1

format:
    html:
        toc: true
        toc-depth: 4
        toc_expand: true
        embed-resources: false
        code-fold: true
        code-summary: 'Show the code'
        keep-md: false
        grid:
            sidebar-width: 0px
            body-width: 4000px
            margin-width: 200px
            gutter-width: 1.5rem    
---

```{r}
#| label: setup
#| include: false

library(openalexR)
library(dplyr)
library(arrow)
library(ggplot2)

library(IPBES.R)
library(htmlwidgets)
library(htmltools)

library(tictoc)

snowball <- function(backgroundMessage, collect = TRUE) {
  result <- list(
    nodes = nodes |>
      dplyr::filter(
        bm == backgroundMessage
      ),
    edges = edges |>
      dplyr::filter(
        bm == backgroundMessage
      )
  )
  if (collect) {
    result <- lapply(result, dplyr::collect)
  }

  return(result)
}

```

# Introduction
IPBES assessments were converted / enhanced into [Linked Open Data (LOD)](https://github.com/IPBES-Data/IPBES_LOD) using the ttl format. 

Three assessments available in ttl format are currently:

- the [First Global Assessment](https://github.com/IPBES-Data/IPBES_LOD/blob/main/Global%20Assessment%201/README_GA1.md) (GA1)
- the [Values Assessment](https://github.com/IPBES-Data/IPBES_LOD/tree/main/Values%20Assessment) (VA)
- the [Invasive Alien Species Assessment](https://github.com/IPBES-Data/IPBES_LOD/tree/main/Invasive%20Alien%20Species%20Assessment) (IAS).

# Methods

## Manual fixes to ref list

The following DOIS are wrong in the original csv and were corrected manually: `10.1016/j.ecolmodel.2017,04.005` should be `10.1016/j.ecolmodel.2017.04.005`

## Import References and get OpenAlex Works 

```{r}
#| label: load_key_works

fn <- file.path("output", "key_works.rds")
if (!file.exists(fn)) {
  refs_file <- file.path("input", "Query_Submessage_ref_GA.csv")

  refs <- refs_file |>
    read.csv(
      header = TRUE,
      sep = ",",
      quote = "\"",
      stringsAsFactors = FALSE
    ) |>
    dplyr::mutate(
      bm = gsub("http://ontology.ipbes.net/report/subm/", "", Sub.Message),
      Background.Message = NULL,
      doi = gsub("https://doi.org/", "", doi),
      X = NULL
    ) |>
    dplyr::select(
      bm,
      zotero,
      doi
    ) |>
    tibble::as_tibble()

  key_works <- openalexR::oa_fetch(
    entity = "works",
    doi = refs$doi[refs$doi != ""] |> unique(),
    options = list(
      # select = c(
      #     "id",
      #     "doi",
      #     "authorships",
      #     "abstract_inverted_index",
      #     "publication_year"
      # )
    ),
    verbose = TRUE
  )

  refs$doi <- ifelse(
    refs$doi == "",
    as.character(NA),
    paste0("https://doi.org/", refs$doi)
  )

  refs |>
    dplyr::distinct() |>
    dplyr::left_join(
      y = key_works,
      by = "doi"
    ) |>
    saveRDS(
      file = fn
    )
}

key_works <- readRDS(fn)
```

## Run Snowball Searches for each key-paper individually

```{r}
#| label: snowball_search

kws <- key_works |>
  dplyr::group_by(id) |>
  dplyr::summarise(
    bms = list(bm = bm),
    .groups = "keep"
  ) |>
  dplyr::ungroup() |>
  dplyr::mutate(
    id = gsub(
      pattern = "https://openalex.org/",
      replacement = "",
      x = id
    )
  )

dir.create(file.path("output", "snowballs"), showWarnings = FALSE)

tic()

pbapply::pblapply(
  seq_along(kws$id[!is.na(kws$id)]),
  function(i) {
    tic()
    fn <- file.path("output", "snowballs", paste0(kws$id[i], ".rds"))
    if (file.exists(fn)) {
      message("Skippig ", i, " of ", nrow(kws), " - file exists")
    } else {
      message("Snowballing ", kws$id[i], " --- ", i, " of ", nrow(kws), " ...")

      try(
        kws$id[[i]] |>
          openalexR::oa_snowball() |>
          saveRDS(file = fn)
      )
    }
    toc()
  }
) |>
  invisible()

toc()
```

## Convert to Parquet Database

```{r}
#| label: convert_to_parquet
#|

path_nodes <- file.path("output", "nodes")
path_edges <- file.path("output", "edges")

if (!(dir.exists(path_nodes) & dir.exists(path_edges))) {
  dir.create(path_nodes, showWarnings = FALSE)
  dir.create(path_edges, showWarnings = FALSE)

  for (bm1 in unique(key_works$bm)) {
    message("\nProcessing ", bm1, " - ", which(unique(key_works$bm) == bm1), " of ", length(unique(key_works$bm)))
    tic()
    ids <- key_works |>
      dplyr::filter(
        bm == bm1,
        id != is.na(id)
      ) |>
      dplyr::select(
        id
      ) |>
      dplyr::mutate(
        id = gsub(
          pattern = "https://openalex.org/",
          replacement = "",
          x = id
        )
      ) |>
      unlist() |>
      unique()

    ### Loading
    message("  |- Loading files")
    snowballs <- pbapply::pblapply(
      ids,
      function(id) {
        result <- NULL
        try(
          result <- readRDS(file.path("output", "snowballs", paste0(id, ".rds"))),
          silent = TRUE
        )
        return(result)
      }
    )

    ### Converting and saving nodes
    message("  |- Combining and writing nodes")
    pbapply::pblapply(
      snowballs,
      function(x) {
        if (is.null(x)) {
          node <- NULL
        } else {
          node <- x[["nodes"]]
          node$author <- IPBES.R::abbreviate_authors(node)
          if (!"ab" %in% colnames(node)) {
            node <- node |>
              dplyr::mutate(
                ab = as.character(NA)
              )
          }
          if (!"doi" %in% colnames(node)) {
            node <- node |>
              dplyr::mutate(
                doi = as.character(NA)
              )
          }
          node <- node |>
            dplyr::mutate(
              grants = NULL,
              counts_by_year = NULL,
              referenced_works = NULL,
              related_works = NULL,
              concepts = NULL,
              topics = NULL
            )
          #     dplyr::select(
          #       id,
          #       doi,
          #       author,
          #       publication_year,
          #       title,
          #       ab,
          #       oa_input
          #     )
        }
        return(node)
      }
    ) |>
      dplyr::bind_rows() |>
      dplyr::distinct() |>
      dplyr::mutate(
        bm = bm1
      ) |>
      arrow::write_dataset(
        path = path_nodes,
        format = "parquet",
        partitioning = c("bm"),
        existing_data_behavior = "delete_matching"
      )

    ### Converting and saving edges
    message("  |- Combining and writing edges")
    pbapply::pblapply(
      snowballs,
      function(x) {
        x[["edges"]]
      }
    ) |>
      dplyr::bind_rows() |>
      dplyr::distinct() |>
      dplyr::mutate(
        bm = bm1
      ) |>
      arrow::write_dataset(
        path = path_edges,
        format = "parquet",
        partitioning = c("bm"),
        existing_data_behavior = "delete_matching"
      )
    toc()
  }
}

nodes <- arrow::open_dataset(
  sources = path_nodes
)

edges <- arrow::open_dataset(
  sources = path_edges
)
```


## Extract number of references per background message per year

```{r}
#| label: refs_per_year

refs_per_year <- nodes |>
  dplyr::group_by(
    bm,
    publication_year
  ) |>
  dplyr::summarise(
    n = dplyr::n()
  ) |>
  dplyr::arrange(
    bm,
    publication_year
  ) |>
  dplyr::collect()
```

## Figures

### Publications per year

```{r}
#| label: fig_pub_per_year
#|

figname <- file.path("figures", "fig_pub_per_year")
if (length(list.files("figures", "fig_pub_per_year")) < 2) {
  fig <- refs_per_year |>
    dplyr::collect() |>
    ggplot(
      aes(
        x = publication_year,
        y = n,
        group = bm,
        color = bm
      )
    ) +
    geom_line() +
    geom_point() +
    geom_vline(
      xintercept = 2019,
      linetype = "dashed"
    ) +
    theme_minimal() +
    labs(
      title = "Number of New Publications per Year by Background Message",
      x = "Publication Year",
      y = "Number of Publications",
      color = "Background Message"
    ) +
    theme(
      legend.position = "bottom",
      legend.title = element_blank()
    ) +
    xlim(2000, 2025) +
    scale_y_log10()

  ggplot2::ggsave(
    filename = paste0(figname, ".png"),
    plot = fig,
    width = 10,
    height = 20
  )
  ggplot2::ggsave(
    filename = paste0(figname, ".pdf"),
    plot = fig,
    width = 10,
    height = 20
  )
  ggplot2::ggsave(
    filename = paste0(figname, ".svg"),
    plot = fig,
    width = 10,
    height = 20
  )
}
```

## Overlap


```{r}
#| label: key_paper_overlap
#|

fn <- file.path("output", "key_paper_overlap.rds")

if (!file.exists(fn)) {
  overlap_keypapers <- nodes |>
    dplyr::filter(
      oa_input,
      !is.na(id)
    ) |>
    dplyr::group_by(id) |>
    dplyr::collect() |>
    dplyr::summarise(
      n = n(),
      author,
      title,
      doi = gsub("https://doi.org/", "", doi),
      bms = list(bm)
    ) |>
    dplyr::distinct() |>
    dplyr::arrange(
      desc(n)
    ) |>
    dplyr::filter(
      n > 1,
      !is.na(id)
    ) |>
    dplyr::mutate(
      id = paste0('<a href="https://openalex.org/', id, '" target="_blank">', id, "</a>"),
      doi = paste0('<a href="https://doi.org/', doi, '" target="_blank">', doi, "</a>")
    ) |>
    dplyr::select(
      n,
      id,
      author,
      title,
      doi,
      bms
    )
    
    saveRDS(
      overlap_keypapers,
      file = fn
    )

  overlap_keypapers |>
    IPBES.R::table_dt(
      fixedColumns = list(leftColumns = 2)
    ) |>
    htmlwidgets::saveWidget(
      file = file.path("output", "key_paper_overlap.html"),
      selfcontained = FALSE
    )
}

```

```{r}
#| label: after_2018_overlap
#|

fn <- file.path("output", "after_2018_overlap.rds")

if (!file.exists(fn)) {
  after_2018_overlap <- nodes |>
    dplyr::filter(
      !oa_input,
      !is.na(id),
      publication_year > 2018
    ) |>
    dplyr::group_by(id) |>
    collect() |>
    summarise(
      n = n(),
      author,
      title,
      doi = gsub("https://doi.org/", "", doi),
      bms = list(bm)
    ) |>
    dplyr::distinct() |>
    dplyr::arrange(
      desc(n)
    ) |>
    dplyr::filter(
      n > 2,
      !is.na(id)
    ) |>
    dplyr::mutate(
      id = paste0('<a href="https://openalex.org/', id, '" target="_blank">', id, "</a>"),
      doi = paste0('<a href="https://doi.org/', doi, '" target="_blank">', doi, "</a>")
    ) |>
    dplyr::select(
      n,
      id,
      author,
      title,
      doi,
      bms
    )
    
    saveRDS(
      after_2018_overlap,
      file = fn
    )

  after_2018_overlap |>
    IPBES.R::table_dt(
      fixedColumns = list(leftColumns = 2)
    ) |>
    htmlwidgets::saveWidget(
      file = file.path("output", "after_2018_overlap.html"),
      selfcontained = FALSE
    )
}

```

# Results

The dataset contains **`r key_works |> select(zotero) |> distinct() |> collect() |> nrow()`** paper referenced in the background messages. Out of these, **`r nodes |> filter(oa_input) |> select(id) |> distinct() |> collect() |> nrow()`** were usable for the snowball search and also yielded results.

```{r}
#| label: numbers_table
#| 

nodes |>
  dplyr::group_by(bm) |>
  dplyr::summarise(
    n = dplyr::n(),
    n_key_paper = sum(oa_input == TRUE, na.rm = TRUE),
    n_non_key_paper = sum(oa_input == FALSE, na.rm = TRUE),
    n_nkp_pre_2018 = sum(((oa_input == FALSE) & (publication_year <= 2018)), na.rm = TRUE), # Count of papers with publication_year <= 2018
    n_nkp_post_2018 = sum(((oa_input == FALSE) & (publication_year > 2018)), na.rm = TRUE) # Count of papers with publication_year > 2018
  ) |>
  dplyr::arrange(
    bm
  ) |>
  dplyr::collect() |>
  IPBES.R::table_dt()
```

## Publication types after 2018

Here we show the different types of publication after 2018 as identified by the snowbal search. All Sub-background messages are grouped.

```{r}
#| label: publication_types_grouped
#| echo: false


nodes |>
  dplyr::filter(
    publication_year > 2018
  ) |>
  dplyr::select(
    type
    ) |>
  dplyr::summarize(
    n = n(),
    .by = type
  ) |>
  dplyr::arrange(
    dplyr::desc(n)
  ) |>
  dplyr::collect() |>
  knitr::kable()
```

This table allows the filtering shows the number of work types per sub-background message. It allows filtering as well as sorting and downloading.

```{r}
#| label: publication_types_non_grouped
#| echo: false

nodes |>
  dplyr::filter(
    publication_year > 2018
  ) |>
  dplyr::select(
    bm,
    type
    ) |>
  dplyr::summarize(
    n = n(),
    .by = c(type, bm)
  ) |>
  dplyr::arrange(
    dplyr::desc(n)
  ) |>
  dplyr::collect() |>
  IPBES.R::table_dt(
    fixed_columns = NULL
  )
```
## Publications per year

![](figures/fig_pub_per_year.png)

- [Download `png`](figures/fig_pub_per_year.png)
- [Download `pdf`](figures/fig_pub_per_year.pdf)
- [Download `svg`](figures/fig_pub_per_year.svg)

## Overlap between background messages

In this section we will look at the number of background messages in which a given paper, as identified by their OpenAlex id. We willlook at that separatr for key-papers and non key-papers.

In the following tables only the papers which occur in more than one background message are included and only papers which were published after 2018.

This overlap can have different causes. Either there is an overlap of the key-papers, ehich results in an overlap of the new publications, or there is a topical overlap betweent the new paper themselves. 

The first table shows the overlap of the key-papers only.


<iframe src="output/key_paper_overlap.html" width="100%" height="500px"></iframe>

The second table shows the overlap of all papers after 2018.



<iframe src="output/after_2018_overlap.html" width="100%" height="500px"></iframe>


```{r}
#| eval: false

x <- nodes |>
  dplyr::filter(
    !oa_input,
    !is.na(id)
  ) |>
  dplyr::group_by(id) |>
  dplyr::collect() |>
  summarise(
    bms = list(bm),
    author,
    title,
    doi,
    n = n()
  ) |>
  dplyr::distinct() |>
  dplyr::arrange(
    desc(n)
  ) |>
  dplyr::filter(
    n > 1
  )

dn <- file.path("output", "dois")

if (!dir.exists(dn)) {
  dir.create(dn)
  all_bms <- nodes |>
    dplyr::select(
      bm
    ) |>
    dplyr::distinct(
      bm
    ) |>
    collect() |>
    unlist() |>
    unique()

  for (bms in all_bms) {
    nodes |>
      dplyr::filter(
        bm == bms
      ) |>
      dplyr::select(
        doi
      ) |>
      dplyr::collect() |>
      unlist() |>
      unique() |>
      writeLines(
        con = file.path(dn, paste0(bms, ".txt"))
      )
  }
}
```

